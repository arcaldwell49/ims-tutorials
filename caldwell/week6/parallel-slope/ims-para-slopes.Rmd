---
title: "Week 6 (Part 1) - Parallel Slopes"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
# load packages ----------------------------------------------------------------

library(learnr)
library(dplyr)
library(ggplot2)
library(broom)

# knitr options ----------------------------------------------------------------

knitr::opts_chunk$set(fig.align = "center", 
                      fig.height = 3, 
                      fig.width = 5,
                      echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE)

# Load datasets from local RDS files instead of openintro package
# Note: mpg comes from ggplot2 and is loaded automatically
mariokart <- readRDS("data/mariokart.rds")
babies <- readRDS("data/babies.rds")

# data prep --------------------------------------------------------------------

mpg_manuals <- mpg |>
  filter(grepl("manual", trans))

mariokart <- filter(mariokart, total_pr < 100)

# Required exercises for progress tracking
REQUIRED_EXERCISES <- c(
  "ex1",
  "ex2",
  "ex3",
  "ex4"
)

# Hash generation helpers
# Should ideally be loaded from the imstutorials package when it exists
is_server_context <- function(.envir) {
  # We are in the server context if there are the follow:
  # * input - input reactive values
  # * output - shiny output
  # * session - shiny session
  #
  # Check context by examining the class of each of these.
  # If any is missing then it will be a NULL which will fail.
  
  inherits(.envir$input, "reactivevalues") &
    inherits(.envir$output, "shinyoutput") &
    inherits(.envir$session, "ShinySession")
}

check_server_context <- function(.envir) {
  if (!is_server_context(.envir)) {
    calling_func <- deparse(sys.calls()[[sys.nframe() - 1]])
    err <- paste0("Function `", calling_func, "`", " must be called from an Rmd chunk where `context = \"server\"`")
    stop(err, call. = FALSE)
  }
}
encoder_logic <- function(strip_output = FALSE) {
  p <- parent.frame()
  check_server_context(p)
  # Make this var available within the local context below
  assign("strip_output", strip_output, envir = p)
  # Evaluate in parent frame to get input, output, and session
  local(
    {
      encoded_txt <- shiny::eventReactive(
        input$hash_generate,
        {
          # shiny::getDefaultReactiveDomain()$userData$tutorial_state
          state <- learnr:::get_tutorial_state()
          shiny::validate(shiny::need(length(state) > 0, "No progress yet."))
          shiny::validate(shiny::need(nchar(input$name) > 0, "No name entered."))
          shiny::validate(shiny::need(nchar(input$studentID) > 0, "Please enter your student ID"))
          user_state <- purrr::map_dfr(state, identity, .id = "label")
          user_state <- dplyr::group_by(user_state, label, type, correct)
          user_state <- dplyr::summarize(
            user_state,
            answer = list(answer),
            timestamp = dplyr::first(timestamp),
            .groups = "drop"
          )
          user_state <- dplyr::relocate(user_state, correct, .before = timestamp)
          user_info <- tibble(
            label = c("student_name", "student_id"),
            type = "identifier",
            answer = as.list(c(input$name, input$studentID)),
            timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S %Z", tz = "UTC")
          )
          learnrhash::encode_obj(bind_rows(user_info, user_state))
        }
      )
      output$hash_output <- shiny::renderText(encoded_txt())
    },
    envir = p
  )
}

hash_encoder_ui <- {
  shiny::div("If you have completed this tutorial and are happy with all of your", "solutions, please enter your identifying information, then click the button below to generate your hash", textInput("name", "What's your name?"), textInput("studentID", "What is your student ID?"), renderText({
    input$caption
  }), )
}

# Progress checker logic
progress_checker_logic <- function() {
  p <- parent.frame()
  check_server_context(p)
  local(
    {
      # Reactive to check progress when button is clicked
      progress_info <- shiny::eventReactive(
        input$check_progress,
        {
          state <- learnr:::get_tutorial_state()
          
          # Get labels of completed exercises (any submission counts)
          completed_labels <- names(state)
          
          # Check which required exercises are completed
          completed_required <- REQUIRED_EXERCISES %in% completed_labels
          n_completed <- sum(completed_required)
          n_total <- length(REQUIRED_EXERCISES)
          pct <- round(100 * n_completed / n_total)
          
          # Build detailed status
          missing <- REQUIRED_EXERCISES[!completed_required]
          
          list(
            n_completed = n_completed,
            n_total = n_total,
            pct = pct,
            missing = missing
          )
        }
      )
      
      # Render the progress output
      output$progress_output <- shiny::renderUI({
        info <- progress_info()
        
        # Color based on completion
        bar_color <- if (info$pct == 100) {
          "#28a745"  # green
        } else if (info$pct >= 75) {
          "#ffc107"  # yellow
        } else {
          "#dc3545"  # red
        }
        
        # Build the UI
        shiny::tagList(
          shiny::tags$div(
            style = "margin-bottom: 10px; font-weight: bold;",
            sprintf("Progress: %d / %d exercises completed (%d%%)", 
                    info$n_completed, info$n_total, info$pct)
          ),
          # Progress bar
          shiny::tags$div(
            style = "background-color: #e9ecef; border-radius: 4px; height: 24px; margin-bottom: 15px;",
            shiny::tags$div(
              style = sprintf(
                "background-color: %s; width: %d%%; height: 100%%; border-radius: 4px; transition: width 0.3s;",
                bar_color, info$pct
              )
            )
          ),
          # Show missing exercises if any
          if (length(info$missing) > 0) {
            shiny::tags$div(
              style = "color: #856404; background-color: #fff3cd; border: 1px solid #ffeeba; padding: 10px; border-radius: 4px;",
              shiny::tags$strong("Missing exercises:"),
              shiny::tags$ul(
                lapply(info$missing, function(x) shiny::tags$li(x))
              )
            )
          } else {
            shiny::tags$div(
              style = "color: #155724; background-color: #d4edda; border: 1px solid #c3e6cb; padding: 10px; border-radius: 4px;",
              shiny::tags$strong("All required exercises completed! You're ready to generate your hash.")
            )
          }
        )
      })
    },
    envir = p
  )
}
```

## Welcome

Welcome to this interactive tutorial on parallel slopes models! In this lesson, you'll extend your understanding of simple linear regression to handle more complex real-world scenarios where multiple factors influence an outcome.

### Learning Objectives

By the end of this tutorial, you will be able to:

1. **Explain the concept of parallel slopes models** and when they are appropriate for analyzing data with both numeric and categorical explanatory variables

2. **Fit parallel slopes models in R** using the `lm()` function with multiple explanatory variables

3. **Interpret coefficients** in parallel slopes models, understanding what they tell us about the relationship between variables after controlling for other factors

4. **Visualize parallel slopes models** as multiple parallel lines in a scatterplot and understand the geometric interpretation

5. **Translate between three representations** of regression models: mathematical equations, geometric visualizations, and R syntax

6. **Apply parallel slopes models** to real-world datasets to answer questions about relationships between variables while accounting for potential confounding factors

### Why Parallel Slopes?

Simple linear regression is powerful, but real data often involves multiple factors. For instance, does fuel economy improve over time due to engineering advances, or simply because consumers bought smaller cars? Parallel slopes models help us disentangle these effects by controlling for multiple variables simultaneously.

Let's get started!

## Motivating Example: What if we have two groups?

In simple linear regression, we model a response variable using a single explanatory variable. But what happens when we need to account for multiple factors? Let's explore this with a real example about fuel efficiency.

### Fuel efficiency by engine size

OK, let's consider a situation in which simple linear regression might not be sophisticated enough to suit our needs. This scatterplot shows the relationship between highway fuel economy and engine size for 77 configurations of manual transmission cars popular from 1999 to 2008. It appears as though there is a negative relationship between engine size and fuel economy---which should make sense: bigger engines tend to go in bigger cars, which tend to be heavier, and which tend to get worse mileage. 

We could certainly fit a linear regression line through these points, but that would only tell us part of the story.

```{r fuel-efficiency-size, echo=TRUE}
ggplot(data = mpg_manuals, aes(x = displ, y = hwy)) + 
  geom_point()
```

### Fuel efficiency over time

Did fuel economy improve over time? These side-by-side boxplots suggest that it might have. But here's the key question: does this improvement represent a feat of engineering? Or merely a change in consumer taste (e.g., people buying smaller cars)? 

**The Challenge:** How do we know that the increase in fuel economy was not just due to the cars in 2008 generally having smaller engines --- which we've already observed to be associated with greater fuel economy?

This is where parallel slopes models become essential.


```{r fuel-efficiency-time, echo=TRUE}
ggplot(data = mpg_manuals, aes(x = factor(year), y = hwy)) + 
  geom_boxplot()
```

### A parallel slopes model

What we really want is a model that will assess the effects of engine size and year simultaneously. That is, we want to understand the effect of time on fuel economy, after controlling for engine size.

Here we see a visual depiction of a parallel slopes model. These models occur when one of the explanatory variables is numeric, and the other is categorical. In this case, the year variables has two levels, and the model accordingly consists of two parallel lines.

```{r mpg-displ-resid}
mod <- lm(hwy ~ displ + factor(year), data = mpg)

ggplot(data = mpg, aes(x = displ, y = hwy, color = factor(year))) + 
  geom_line(data = augment(mod), aes(y = .fitted, color = `factor(year)`)) + 
  geom_segment(data = filter(augment(mod), hwy <= 25 & displ > 6.1 & displ < 7), 
               aes(xend = displ, yend = .fitted, color = `factor(year)`), 
               arrow = arrow(length = unit(0.3,"cm")), 
               size = 1) + 
  geom_point()
```

### Adding a new variable

Consider:

$$
hwy = \beta_0 + \beta_1 \cdot displ + \beta_2 \cdot year + \epsilon
$$

Multiple regression allows us to build such models by simply adding another variable---and another coefficient---to our model.

### Adding a new variable in R

As you might suspect, telling R about the second variable in our regression model is just as easy. As you might expect, telling R about the second variable in our regression model is straightforward. We simply add another term to the right-hand side of the formula using the `+` operator. 

**Important:** Here we must ensure that R interprets `year` as a categorical variable by wrapping it in `factor()`, since it's encoded numerically but represents distinct time periods, not a continuous quantity.

```{r adding-a-variable, echo=TRUE}
lm(hwy ~ displ + factor(year), data = mpg)
```

### Fitting a parallel slopes model 

We use the `lm()` function to fit linear models to data. In this case, we want to understand how the price of MarioKart games sold at auction varies as a function of not only the number of wheels included in the package, but also whether the item is new or used. Obviously, it is expected that you might have to pay a premium to buy these new. But how much is that premium? Can we estimate its value *after controlling for the number of wheels*?

We will fit a parallel slopes model using `lm()`. In addition to the `data` argument, `lm()` needs to know which variables you want to include in your regression model, and how you want to include them. It accomplishes this using a `formula` argument. A simple linear regression formula looks like `y ~ x`, where `y` is the name of the response variable, and `x` is the name of the explanatory variable. Here, we will simply extend this formula to include multiple explanatory variables. A parallel slopes model has the form `y ~ x + z`, where `z` is a categorical explanatory variable, and `x` is a numerical explanatory variable. 

The output from `lm()` is a model object, which when printed, will show the fitted coefficients.

- The dataset `mariokart` is already loaded for you. Take a peek at the data using `glimpse()`. (See also the details of the [`mariokart` data set](https://www.openintro.org/data/index.php?data=mariokart) on our data set page.)
- Use `lm()` to fit a parallel slopes model for total price as a function of the number of wheels and the condition of the item. Use the argument `data` to specify the dataset you're using.

```{r ex1, exercise=TRUE, exercise.cap="ex1"}
# Explore the data


# Fit parallel slopes

```


```{r ex1-hint-1}
# Explore the data
glimpse(mariokart)
```

```{r ex1-hint-2}
# Fit parallel slopes
lm(total_pr ~ ___ + ___, data = mariokart)
```

```{r ex1-solution}
# Explore the data
glimpse(mariokart)

# Fit parallel slopes
lm(total_pr ~ wheels + cond, data = mariokart)
```

### Reasoning about two intercepts

The `mariokart` data contains several other variables. The `total_pr`, `start_pr`, and `ship_pr` variables are numeric, while the `cond` and `stock_photo` variables are categorical. 

*Hint:* Remember, a parallel slopes model uses one numeric variable and one categorical variable.


```{r mc1, echo=FALSE}
question("Which `formula` will result in a parallel slopes model?",
  answer("`total_pr ~ start_pr + ship_pr`", message = "Which variable is categorical?"),
  answer("`cond ~ start_pr + stock_photo`", message = "Is the response numeric?"),
  answer("`total_pr ~ ship_pr + stock_photo`", correct = TRUE),
  answer("`total_pr ~ cond`", message = "How many explanatory variables?"), 
  allow_retry = TRUE
)
```

## Visualizing parallel slopes models

### Three variables, one plot

In this scatterplot, we use color to differentiate the cars from 2008 from those in 1999. Do you notice anything about the green points relative to the red points? 

In this manner, we have depicted three variables---two numeric and one categorical---on the same scatterplot. Thus, this plot will enable us to visualize our parallel slopes model in the data space.


```{r mpg-data-space, echo=TRUE}
data_space <- ggplot(data = mpg_manuals, aes(x = displ, y = hwy, color = factor(year))) + 
  geom_point()
data_space
```



### Setting up the model


- Define 
$$newer = \begin{cases} 1 & \text{if } year = 2008 , \\\\  0 & \text{if } year = 1999 \end{cases}$$ 


- Our model is:  
$$ \hat{hwy} = \hat{\beta}_0 + \hat{\beta}_1 \cdot displ + \hat{\beta}_2 \cdot newer $$ 



We're going to make use of a little high school algebra to inform our understanding of the geometry of our model. 

First, since our categorical explanatory variable `year` has only two levels, we're going to define a binary variable called `newer` that takes on the value 1 for cars from 2008, and is 0 otherwise. 

Then, we can express our model mathematically using the following equation.

### Two vintages of cars

- For `year = 2008`, we have 
$$ \hat{hwy} = 35.276 - 3.611 \cdot displ + 1.402 \cdot (1) =$$ $$= \left( 35.276 + 1.402 \right) - 3.611 \cdot displ $$ 

- For `year = 1999`, we have 
$$ \hat{hwy}  = 35.276 - 3.611 \cdot displ + 1.402 \cdot (0) =$$  $$ = 35.276 - 3.611 \cdot displ $$ 

We compute the fitted coefficients using `lm()`. 

```{r echo=TRUE}
mod <- lm(hwy ~ displ + factor(year), data = mpg)
mod
```

So what happens when the cars are newer? Plugging in and simplifying reveals the equation for a line. Note that since `displ`acement is our numeric explanatory variable, the slope of the line is -3.611 mpg per litre, and the intercept is 36.678---the sum of the other two coefficients.

What about the older cars from 1999? In that case, the value of `newer` is 0, and plugging that in to our equation also results in the equation for a line. This line also has a slope of -3.611 mpg per litre, but now the intercept is just 35.276 mpg.


### Two parallel lines


$$
\begin{aligned}
    \hat{hwy} &= ( \hat{\beta}_0 + \hat{\beta}_2 ) + \hat{\beta}_1 \cdot displ \\\\
    &= \left( 35.276 + 1.402 \right) - 3.611 \cdot displ \\\\
    &= \, \stackrel{intercept}{36.678} - \stackrel{slope}{3.611} \cdot displ
\end{aligned}
$$


$$
\begin{aligned}
    \hat{hwy} &= \hat{\beta}_0 + \hat{\beta}_1 \cdot displ \\\\
    &= 35.276 - 3.611 \cdot displ \\\\
    &= \, \stackrel{intercept}{35.276} - \stackrel{slope}{3.611} \cdot displ 
\end{aligned}
$$


Thus, our model consists of two parallel lines: one for newer cars from 2008, and one for older cars from 1999. The two lines are parallel because they have the same slope, but they are not the same line, because they have different intercepts. 

This is why models with one numeric explanatory variable and one categorical explanatory variable are called parallel slopes models. 

### Retrieving the coefficients

```{r eval=FALSE, echo=TRUE}
augment(mod)
```

```
    ##     hwy displ `factor(year)`  .fitted   .se.fit      .resid        .hat
    ## 1    29   1.8         1999 28.77593 0.4522966  0.22406921 0.014314273
    ## 2    29   1.8         1999 28.77593 0.4522966  0.22406921 0.014314273
    ## 3    31   2.0         2008 29.45587 0.4753645  1.54412984 0.015811613
    ## 4    30   2.0         2008 29.45587 0.4753645  0.54412984 0.015811613
    ## 5    26   2.8         1999 25.16494 0.3617297  0.83505537 0.009155689
    ## 6    26   2.8         1999 25.16494 0.3617297  0.83505537 0.009155689
    ## 7    27   3.1         2008 25.48379 0.3661035  1.51621462 0.009378436
    ## 8    26   1.8         1999 28.77593 0.4522966 -2.77593079 0.014314273
    ## 9    25   1.8         1999 28.77593 0.4522966 -3.77593079 0.014314273
    ## 10   28   2.0         2008 29.45587 0.4753645 -1.45587016 0.015811613
```


In order to visualize our model, we need to extract the necessary information about our model that was created by `lm()`. Conceptually, the three fitted coefficients will give us that information. However, it's easier in ggplot to simply plot the fitted values and connect them with a line. This process is  streamlined by the `augment()` function from the broom package. 

Applying `augment()` to our model will return a data frame with the fitted values attached, like the one you see here. Note that the name for the variable that contains the fitted values is `.fitted`, and the name of the categorical variable is ``factor(year)``.


### Parallel lines on the scatterplot

Finally, we can use the `geom_line()` function to put the two lines on the scatterplot that we created previously. We need to tell `geom_line()` to plot the fitted values, rather than the observed values, and those values only exist in the augmented model object that we created previously.



```{r mpg-data-parallel, echo=TRUE}
data_space +
  geom_line(data = augment(mod), aes(y = .fitted, color = `factor(year)`))
```

### Using `geom_line()` and `augment()`

Parallel slopes models are so-named because we can visualize these models in the data space as not one line, but two *parallel* lines. To do this, we'll draw two things:

- a scatterplot showing the data, with color separating the points into groups
- a line for each value of the categorical variable

Our plotting strategy is to compute the fitted values, plot these, and connect the points to form a line. The `augment()` function from the **broom** package provides an easy way to add the fitted values to our data frame, and the `geom_line()` function can then use that data frame to plot the points and connect them. 

Note that this approach has the added benefit of automatically colouring the lines appropriately to match the data. 

### Your turn

You already know how to use `ggplot()` and `geom_point()` to make the scatterplot. The only twist is that now you'll pass your `augment()`-ed model as the `data` argument in your `ggplot()` call. When you add your `geom_line()`, instead of letting the `y` aesthetic inherit its values from the `ggplot()` call, you can set it to the `.fitted` column of the `augment()`-ed model. This has the advantage of automatically colouring the lines for you.

Coming back to the `mariokart` data, try to plot the parallel slope model:

The parallel slopes model `mod` relating total price (`total_pr`) to the number of `wheels` and `cond`ition is already in your workspace.

- Call the `augment()` function on the model object `mod`, then use `glimpse(augmented_mod)` to explore the resulting data frame.
- To create `data_space` first map each of the three variables to an aesthetic in your `ggplot()`, then use the `geom_point()` function.
- To plot the fitted values as a line, your `geom_line()` call will need to specify a new `y` aesthetic to plot the `.fitted` values.

```{r ex2-setup}
mod <- lm(total_pr ~ wheels + cond, data = mariokart)
```

```{r ex2, exercise=TRUE, exercise.cap="ex2"}
# Augment the model
augmented_mod <-___
glimpse(___)

# scatterplot, with color
data_space <- ggplot(___, aes(x = ___, y = ___, color = ___)) + 
  geom_point()
  
# single call to geom_line()
data_space + 
  geom_line(aes(y = ___))
```

```{r ex2-hint-1}
augmented_mod <- augment(mod)
glimpse(augmented_mod)
```

```{r ex2-hint-2}
data_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr, color = cond)) + 
  geom_point()
```

```{r ex2-solution}
# Augment the model
augmented_mod <- augment(mod)
glimpse(augmented_mod)

# scatterplot, with color
data_space <- ggplot(augmented_mod, aes(x = wheels, y = total_pr, color = cond)) + 
  geom_point()
  
# single call to geom_line()
data_space + 
  geom_line(aes(y = .fitted))
```

## Interpreting parallel slopes coefficients

### Intercept interpretation

Most often, our primary interest in models like this one is interpreting the value of the coefficients. What does the model tell us about the relationship between fuel economy and engine size in the context of the year when the cars were manufactured? 

```{r echo=TRUE}
lm(hwy ~ displ + factor(year), data = mpg)
```

Let's start with the main intercept. The value is 35.276 and the units are the same as those of the response variable---miles per gallon. Recall that this is the expected fuel economy for a car from 1999 that had an engine size of 0 litres. Of course, in this case, this value has little meaning, since there is no such thing as a car with an engine size of 0 litres, but that is the literal interpretation of the role that 35.276 plays in our model. 

As we saw above, the coefficient on year can also be thought of as an intercept. Note here that R has chosen to report the name of the coefficient as `factor(year)2008`. This reflects the fact that `year` was the name of the variable we gave R, `factor(year)` was the formatting used in the `lm()` command, and 2008 is the value of that variable about which R is reporting. This variable is identical to the newer variable that we defined in the previous sections. Here, R is telling us that cars manufactured in 2008 get about 1.4 miles per gallon better gas mileage than those manufactured in 1999, after controlling for engine size. This is our key finding, and we'll return to this in just a minute.

### Slope interpretation

How did we "control for engine size"? 

```{r echo=TRUE}
lm(hwy ~ displ + factor(year), data = mpg)
```

By including the `displ` variable in our model, we also obtain a coefficient for engine size. This is our slope coefficient, and it tells us that each extra litre of engine size is associated with a decrease in expected fuel economy of 3.61 miles per gallon, after controlling for year of manufacture. This is the negative relationship we saw earlier: larger engines tend to go in less fuel efficient cars. The difference now is that we are able to estimate the size of that effect while simultaneously considering the effect of time.


### Avoiding misunderstandings

The interpretation of coefficients in a multiple regression model can get complicated, so let's quickly review a few common quandaries: 

- There is only *one* slope 
- Which is the reference level?
- What are the units?  
- What does it mean to "controlling for"?

First, there is only one slope in the parallel slopes models that we have considered thus far. Yes, there are two explanatory variables, but only the one numeric explanatory variable is associated with a slope. Later, we will consider more complex models that have more than one "slope."

Second, pay careful attention to the reference level of your categorical variables. Every factor in R has a reference level - which you can set - but by default it is the first level alphabetically. 

Third, units are important. Every coefficient has units that relate to the units of the response variable. Intercepts are in the same units as the response variable, and slope coefficients are in the units of the response per unit of the explanatory variable. 

Finally, the key difference in multiple regression is that coefficients must be interpreted in the context of the other explanatory variables. The "after controlling for" phrasing is crucial to having a valid understanding of your model.

### Intercept interpretation

Recall that the `cond` variable of the `mariokart` data is either `new` or `used`. Here are the fitted coefficients from your model:

```{r, echo=TRUE}
lm(total_pr ~ wheels + cond, data = mariokart)
```

Choose the correct interpretation of the slope coefficient:

```{r mc2}
question("Choose the correct interpretation of the coefficient on `condused`:",
  answer("For each additional wheel, the expected price of a used MarioKart is $5.58 lower.", message="Which coefficient are you interpreting?"),
  answer("The expected price of a used MarioKart is $5.58 less than that of a new one with the same number of wheels.", correct = TRUE, message="Right!"),
  answer("The expected price of a new MarioKart is $5.58 less than that of a used one with the same number of wheels.", message="Newer or older?"),
  answer("The used MarioKarts are always $5.58 cheaper.", message="After controlling for..."), 
  allow_retry=TRUE
)
```




### Common slope interpretation


Recall the fitted coefficients from our model:

```{r, echo=TRUE}
lm(total_pr ~ wheels + cond, data = mariokart)
```



```{r mc3}
question("Choose the correct interpretation of the slope coefficient:",
  answer("For each additional wheel, the expected price of a MarioKart increases by $7.23 regardless of whether it is new or used.", correct = TRUE, message="Right!"),
  answer("For each additional wheel, the expected price of a new MarioKart increases by $7.23.", message="Doesn't the condition matter?"),
  answer("The expected price of a used MarioKart is $5.59 less than that of a new one with the same number of wheels.", message="Which coefficient are you interpreting?"),
  answer("You should always expect to pay $42.37 for a MarioKart.", message="Always?"), 
  allow_retry=TRUE
)
```




## Three ways to describe a model


Regression models are abstract concepts, and statisticians have developed three complementary ways to understand them:

- **Mathematical:** Using equations with variables and coefficients
- **Geometric:** Visualizing as lines, planes, or surfaces in data space  
- **Syntactic:** Writing formulas that R can interpret and execute

Understanding all three perspectives will deepen your intuition about how models work. Throughout this tutorial, we'll move fluidly between these representations, showing how each offers unique insights.

Let's explore each approach in detail.



### Mathematical

A multiple regression model can be expressed as an equation for the response variable y in terms of some explanatory variables $x_1$ and $x_2$. The coefficients of the model-- $\beta_0$, $\beta_1$ , and $\beta_2$---allow us to translate our knowledge about x into information about y. A statistical model will always include an error term like epsilon that captures our uncertainty. These errors---which are manifest as residuals---are critical to the process of statistical inference---but that is a subject for a later course. For now, we will focus on the variables and the coefficients.

- Equation: 
$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$

- Residuals: 
$\epsilon \sim N(0, \sigma_\epsilon)$

- Coefficients: 
$\beta_0, \beta_1, \beta_2$


### Geometric

Math isn't everyone's cup of tea, and while this doesn't make it any less important, we will develop geometric intuition about regression models in this tutorial. Our data live in a "space," and we will refer to this as the "data space." In this scatterplot, we view the highway gas mileage of several popular cars along with the corresponding size of their engines---as measured by displacement. Each point on the scatterplot represents an observation. 

A simple linear regression model can be visualized as a line through this data space.

```{r}
ggplot(mpg, aes(x = displ, y = hwy, color = factor(year))) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE)
```


### Syntactic


```{r, echo=TRUE}
lm(hwy ~ displ + factor(year), data = mpg)
```

Finally, R doesn't really understand math or geometry. But of course, R is really good at performing the computations that we will need in order to fit and visualize our models. We will use a special syntax---called a formula---to communicate models to R. This syntax is less specific and more suggestive than a mathematical equation, but can be put into a 1-1 correspondence with it.

### Multiple regression


- $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon$

- `y ~ x1 + x2 + x3`

- one line becomes multiple lines or a plane, or even multiple planes 


As we extend simple linear regression into multiple regression, we will add additional explanatory variables. Instead of just having x, we will have x1 and x2, even possibly even more. The formula syntax will extend naturally, and additional coefficients will make their way into the mathematical equation. 

As we add complexity, the data space will increase from two to three---and even more---dimensions, and the class of geometric objects that we can use to describe models will broaden to include multiple lines, planes, and even multiple planes. 

Unfortunately, while the mathematical and syntactic characterizations will scale easily to an arbitrary number of explanatory variables, human beings are limited in our ability to visually process more than three numeric dimensions. We will get creative to push this boundary as far as we can, but we are doomed to fail.

## Practice: Syntax, math, and plots

Now it's your turn! In the following exercises, you'll practice building parallel slopes models by translating between mathematical notation, visualizations, and R code. These exercises will help solidify your understanding of how these different representations connect.


### Syntax from math

The `babies` data set contains observations about the birthweight and other characteristics of children born in the San Francisco Bay area from 1960--1967 (You will find details of the [babies data](https://www.openintro.org/data/index.php?data=babies) on our [data set page](https://www.openintro.org/data/).). 

**Your Task:** Build a model for birthweight as a function of the mother's `age` and whether this child was her first birth (`parity == 0` means first birth).

**Mathematical Model:**

$$
birthweight = \beta_0 + \beta_1 \cdot age + \beta_2 \cdot parity + \epsilon
$$

**Note:** The birthweight variable is recorded in the column `bwt`.

**Instructions:**

- Use `lm()` to build the parallel slopes model specified above
- The `parity` variable is already coded as binary numeric values (0 or 1), so you don't need to use `factor()`
- Remember that the formula syntax mirrors the mathematical equation: `y ~ x1 + x2`


$$
    birthweight = \beta_0 + \beta_1 \cdot age + \beta_2 \cdot parity + \epsilon
$$

The birthweight variable is recorded in the column `bwt`.

- Use `lm()` to build the parallel slopes model specified above. It's not necessary to use `factor()` in this case as the variable `parity` is coded using binary numeric values.


* The formula syntax is based on the mathematical expression of the model. For example, given the model 
$y = \beta_0 + \beta_1 \cdot x + \beta_2 \cdot z$, the corresponding formula is `y ~ x + z`.


```{r ex3, exercise=TRUE, exercise.cap="ex3"}
# build model

```

```{r ex3-hint}
lm(bwt ~ ___ + ___, data = ___)
```

```{r ex3-solution}
# build model
lm(bwt ~ age + parity, data = babies)
```

### Syntax from plot

This time, we'd like to build a model for birthweight as a function of the length of gestation and the mother's smoking status. Use the plot to inform your model specification.

```{r echo=FALSE}
ggplot(data = babies, aes(x = gestation, y = bwt, color = factor(smoke))) +
  geom_point() + 
  labs(color = "smoke")
```

Use `lm()` to build a parallel slopes model implied by the plot. It's not necessary to use `factor()` in this case either.

```{r ex4, exercise=TRUE, exercise.cap="ex4"}
# build model

```

```{r ex4-hint}
lm(___ ~ ___ + ___, data = babies)
```

```{r ex4-solution}
# build model
lm(bwt ~ gestation + smoke, data = babies)
```



## Congratulations!

You have successfully completed this activity.
You need to generate a hash for submission, click "Next Topic", generate the hash, and submit it on Blackboard.


## Submit

```{r progress-check, echo=FALSE, context="server"}
progress_checker_logic()
```

```{r progress-ui, echo=FALSE}
shiny::actionButton("check_progress", "Check My Progress", 
                    style = "margin-bottom: 15px; background-color: #007bff; color: white;")
shiny::uiOutput("progress_output")
shiny::tags$hr()
```

```{r, echo=FALSE, context="server"}
encoder_logic()
```

```{r encode, echo=FALSE}
learnrhash::encoder_ui(ui_before = hash_encoder_ui)
```